{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Text preprocessing and normalization is crucial before building a proper NLP model. Some of the important steps are:\n",
    "\n",
    "1. converting words to lower/upper case\n",
    "2. removing special characters\n",
    "3. removing stopwords and high/low-frequency words\n",
    "4. stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#rcParams['figure.figsize'] = 30, 60\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing data for text, ideally one column with text strings\n",
    "data = pd.read_excel('Comments.xlsx')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. converting words to lower/upper case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by converting all of the words into a consistent case format, say lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Comments=data.Comments.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the number of words by splitting them by a space\n",
    "words = data.Comments.apply(lambda x: len(x.split(\" \")))\n",
    "words.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating number of words of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Comments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Word Cloud from step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = ''.join(map(str, data.Comments))\n",
    "print(len(word_cloud))\n",
    "wordcloud = WordCloud(max_font_size=200, background_color=\"white\",\\\n",
    "                          scale = 5,width=700, height=400).generate(word_cloud)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_review'] = data.Comments.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "token = [word_tokenize(each) for each in data.Comments]\n",
    "tokens = [item for sublist in token for item in sublist]\n",
    "print(\"Number of unique tokens then: \",len(set(tokens)))\n",
    "\n",
    "token_lists_lower = [word_tokenize(each) for each in data.text_review]\n",
    "tokens_lower = [item for sublist in token_lists_lower for item in sublist]\n",
    "print(\"Number of unique tokens now: \",len(set(tokens_lower)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tokens has gone down just from normalizing the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. removing special characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we will proceed by removing all of the special characters; however, it pays to keep in mind that this is something to revisit depending on the results we get later. The following gives a list of all the special characters in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting non alpha numeric charactes that are not spaces\n",
    "spl_chars = data.text_review.apply(lambda x: [each for each in list(x) if not each.isalnum() and each != ' '])\n",
    "\n",
    "## Getting list of list into a single list\n",
    "flat_list = [item for sublist in spl_chars for item in sublist]\n",
    "\n",
    "## Unique special characters\n",
    "set(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "review_backup = data.text_review.copy()\n",
    "data.text_review = data.text_review.apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our reviews change after removing these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old Review:\")\n",
    "review_backup.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Review:\")\n",
    "data.text_review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique tokens has dropped further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lists = [word_tokenize(each) for each in data.Comments]\n",
    "tokens = [item for sublist in token_lists for item in sublist]\n",
    "print(\"Number of unique tokens then: \",len(set(tokens)))\n",
    "\n",
    "token_lists = [word_tokenize(each) for each in data.text_review]\n",
    "tokens = [item for sublist in token_lists for item in sublist]\n",
    "print(\"Number of unique tokens now: \",len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Cloud for step 2 excluding special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_review = ''.join(map(str, data.text_review))\n",
    "print(len(word_cloud_review))\n",
    "wordcloud = WordCloud(max_font_size=500, background_color=\"white\",\\\n",
    "                          scale = 6,width=1600, height=800).generate(word_cloud_review)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig('wordcloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. removing stopwords and high/low-frequency words (ENGLISH):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords naturally occur very frequently in the English language without adding any context specific insights. It makes sense to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopwords\n",
    "eng_stop_words = stopwords_corpus.words('english')\n",
    "noise_words.extend(eng_stop_words)\n",
    "noise_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. stemming/lemmatization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found.\n",
    "On the other hand, lemmatization takes into consideration the morphological analysis of the words. So lemmatization takes into account the grammar of the word and tries to find the root word instead of just getting to the root word by brute force methods.\n",
    "\n",
    "Now we are ready for the last part of our pre-processing - **stemming & lemmatization**.\n",
    "\n",
    "Different forms of a word often communicate essentially the same meaning. For example, thereâ€™s probably no difference in intent between a search for `shoe` and a search for `shoes`. The same word may also appear in different tenses; e.g. \"run\", \"ran\", and \"running\". These syntactic differences between word forms are called **inflections**. In general, we probably want to treat inflections identically when featurizing our text.\n",
    "\n",
    "Sometimes this process is nearly-reversible and quite safe (e.g. replacing verbs with their infinitive, so that \"run\", \"runs\", and \"running\" all become \"run\"). Other times it is a bit dangerous and context-dependant (e.g. replacing superlatives with their base form, so that \"good\", \"better\", and \"best\" all become \"good\"). The more aggressive you are, the greater the potential rewards and risks. For a very aggressive example, you might choose to replace \"Zeus\" and \"Jupiter\" with \"Zeus\" only; this might be OK if you are summarizing myths, confusing if you are working on astronomy, and disastrous if you are working on comparative mythology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Creating a method for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to create a bag of words from the reviews, excluding the noise words we identified earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a python object of the class CountVectorizer\n",
    "bow_counts = CountVectorizer(tokenizer= word_tokenize, stop_words=noise_words,\n",
    "                             ngram_range=(1, 4))\n",
    "bow_data = bow_counts.fit_transform(data.text_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=bow_counts.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_review_F = ''.join(map(str, final))\n",
    "print(len(word_cloud_review_F))\n",
    "wordcloud_F = WordCloud(max_font_size=500, background_color=\"white\",\\\n",
    "                          scale = 6,width=1600, height=800).generate(word_cloud_review_F)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud_F, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig('wordcloudF.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_counts.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-featurize our original set of reviews based on TF-IDF and split the resulting features into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a python object of the class CountVectorizer\n",
    "### Changes: Removing stop words and including 1-4 grams in the tf-idf data\n",
    "\n",
    "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize,\n",
    "                             ngram_range=(1,4))\n",
    "tfidf_data = tfidf_counts.fit_transform(data.text_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=tfidf_counts.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_review_F = ''.join(map(str, final))\n",
    "print(len(word_cloud_review_F))\n",
    "wordcloud_F = WordCloud(max_font_size=500, background_color=\"white\",\\\n",
    "                          scale = 6,width=1600, height=800).generate(word_cloud_review_F)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud_F, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig('wordcloudF.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
